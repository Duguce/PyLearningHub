# Python机器学习笔记

Abstract：本文是根据作者基于Python学习机器学习相关内容时所做的笔记，主要目的是为了便于面试复习相关知识点。

Author： Duguce

Email：zhgyqc@163.com

Datetime:  2022-04-24 12:47 —— 2022-04-24 12:47



## 1 sklearn

sklearn官网地址：https://scikit-learn.org/stable/

### 1.1 数据集使用

sklearn的数据集API提供了一组简单易用的接口，可以方便地加载、处理和使用数据集。下面介绍一些常用的接口：

**加载内置数据集**

1. `load_boston()`: 加载波士顿房价数据集，返回一个包含房屋价格和13个特征的数据集。
2. `load_iris()`: 加载鸢尾花数据集，返回一个包含鸢尾花品种和4个特征的数据集。
3. `load_digits()`: 加载手写数字数据集，返回一个包含手写数字图像和对应数字的数据集。
4. `load_diabetes()`: 加载糖尿病数据集，返回一个包含患者疾病进展情况和10个特征的数据集。
5. `load_linnerud()`: 加载Linnerud数据集，返回一个包含人体三个生理特征之一和3个运动特征的数据集。

**下载数据集**

1. `fetch_20newsgroups()`: 下载20个新闻组数据集，返回一个包含新闻文章和对应新闻组的数据集。
2. `fetch_lfw_people()`: 下载Labeled Faces in the Wild人脸数据集，返回一个包含人脸图像和对应人名的数据集。
3. `fetch_covtype()`: 下载Covertype数据集，返回一个包含森林植被类型和54个特征的数据集。
4. `fetch_california_housing()`: 下载加利福尼亚房屋价格数据集，返回一个包含房屋价格和8个特征的数据集。
5. `fetch_mldata()`: 下载Mldata数据集，返回一个包含MNIST、CIFAR等常用数据集的数据集。

**数据集对象的常用属性和方法**

无论是加载内置数据集还是下载数据集，Sklearn都会返回一个包含数据和标签的数据集对象。这些对象通常具有以下属性和方法：

1. `data`: 包含数据的numpy数组。
2. `target`: 包含标签的numpy数组。
3. `DESCR`: 数据集的描述信息。
4. `feature_names`: 数据集特征名称的列表。
5. `target_names`: 数据集标签名称的列表。
6. `shape`: 数据集的形状。
7. `keys()`: 返回数据集对象包含的所有键。
8. `values()`: 返回数据集对象包含的所有值。
9. `items()`: 返回数据集对象包含的所有键值对。

以上这些属性和方法都可以用来访问和处理数据集对象的内容。例如，`data`属性可以用来访问数据，`target`属性可以用来访问标签，`DESCR`属性可以用来获取数据集描述信息。同时，`feature_names`和`target_names`属性可以用来访问数据集特征名称和标签名称的列表。

### 1.2 估计器

sklearn的估计器（estimator）接口是一个统一的接口，用于实现各种机器学习算法的训练和预测。估计器接口主要包含以下方法：

**估计器接口的核心方法**

1. `fit(X, y)`: 训练模型，其中X表示输入特征数据，y表示对应的标签。对于有监督学习算法，需要提供X和y，对于无监督学习算法，只需要提供X。
2. `predict(X)`: 预测新样本的标签。其中X表示新样本的特征数据。
3. `transform(X)`: 将输入特征数据X转换为另一个表示，例如降维或特征提取。
4. `fit_transform(X)`: 组合了`fit()`和`transform()`方法，用于训练模型并将输入特征数据X转换为另一个表示。

**估计器接口的其他方法**

1. `score(X, y)`: 计算模型在给定数据集（X和y）上的精度或其他性能指标。
2. `get_params()`: 获取模型的参数及其取值，返回一个字典。
3. `set_params(**params)`: 设置模型的参数值，其中params是一个字典，键是参数名称，值是对应的取值。
4. `get_feature_importances()`: 获取模型中每个特征的重要性分数。
5. `predict_proba(X)`: 预测新样本的标签概率。
6. `decision_function(X)`: 预测新样本的决策函数值。

在使用估计器接口时，通常需要按照以下步骤进行：

1. 创建一个估计器对象。
2. 调用`fit()`方法训练模型，使用训练数据作为输入。
3. 调用`predict()`方法对新样本进行预测。
4. 可选地，使用其他方法对模型进行评估、可视化或调参。

下面是一个简单的示例，使用线性回归模型训练一个数据集并进行预测：

```python
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

# 加载波士顿房价数据集
boston = load_boston()

# 创建线性回归模型
model = LinearRegression()

# 训练模型
model.fit(boston.data, boston.target)

# 预测新样本的房价
new_data = [[0.1, 10.0, 5.0, 0.2, 0.3, 6.0, 40.0, 3.0, 2.0, 300.0, 15.0, 400.0, 20.0]]
prediction = model.predict(new_data)
print(prediction)
```

## 2 分类模型的评估指标

- **准确率（Accuracy）**：分类正确的样本数占总样本数的比例，是最简单直观的评估指标之一。但是，当正负样本不平衡时，准确率可能会失去意义。例如，当负样本占绝大部分时，一个总是预测为负的模型也可以获得很高的准确率。在这种情况下，我们需要结合其他指标一起考虑。
- **精确率（Precision）**：真正例（TP）占预测为正例（TP+FP）的比例，表示预测为正例的样本中有多少是真正的正例。精确率越高，说明模型预测为正例的样本中有越多的真正例，模型的误判率越低。
- **召回率（Recall）**：真正例（TP）占实际为正例（TP+FN）的比例，表示实际为正例的样本中有多少被正确预测为正例。召回率越高，说明模型能够更好地识别正例，模型的漏判率越低。
- **F1值（F1-score）**：精确率和召回率的调和平均数，反映了模型在精确率和召回率之间取得的平衡。F1值越高，说明模型在精确率和召回率之间取得了较好的平衡。

- **ROC曲线和AUC值**：ROC曲线是以假正率（FPR）为横轴，真正率（TPR）为纵轴绘制的曲线。在不同的分类阈值下，TPR和FPR的变化会导致ROC曲线的形态变化。AUC值是ROC曲线下的面积，用于度量模型对正负样本的区分能力。AUC值越高，说明模型的性能越好。
- **混淆矩阵（Confusion Matrix）**：用于可视化分类模型的分类效果。混淆矩阵包括真正例（TP）、假正例（FP）、真反例（TN）和假反例（FN）四种情况，可以用于计算准确率、精确率、召回率和F1值等指标。

## 3 K近邻算法

### 3.1 基本原理

存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前 $k$个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。

K近邻算法的基本原理是基于实例的学习，也称为懒惰学习（lazy learning）。它不会对训练数据进行显式的模型学习，而是在需要预测新样本标签时，从训练数据中选择k个最相似的样本，然后通过它们的标签进行投票，以确定新样本的标签。

具体而言，K近邻算法包含以下几个步骤：

- 计算距离：计算新样本与训练集中每个样本的距离（例如欧几里得距离、曼哈顿距离等）；
- 选择邻居：根据距离选择k个最近的邻居；
- 进行投票：对于分类问题，根据邻居的标签进行投票，并将得票最多的标签作为新样本的标签；对于回归问题，根据邻居的标签计算平均值或加权平均值，并将结果作为新样本的预测值。

K近邻算法的核心思想是“近朱者赤、近墨者黑”，即与某个样本最相似的样本有相似的标签。它的基本假设是局部性，即与某个样本最相似的样本可能位于其周围的局部区域。因此，K近邻算法适用于具有局部性结构的数据集，如在同一区域内分布的样本。

### 3.2 优缺点

**优点：**

- 算法简单、直观、易于理解和实现。K近邻算法不需要训练过程，只需要存储训练数据集，因此非常容易实现。
- 对于数据分布没有假设，适用于任何数据类型。K近邻算法不会对数据的分布做出任何假设，因此适用于任何数据类型，包括非线性数据。
- 在训练集较大的情况下，效果通常很好。K近邻算法在训练集较大的情况下能够取得很好的效果，因为更多的样本可以提供更多的信息。而对于那些样本容量比较小的类域采用这种算法比较容易产生误分类的情况。
- 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属的类别，因此对于类域的交叉或重叠较多的待分类样本集来说，KNN方法较其他方法更为适合。

**缺点：**

- 预测速度较慢，需要计算新样本与所有训练样本的距离。K近邻算法需要计算新样本与所有训练样本之间的距离，因此当训练集较大时，预测速度会变得很慢。
- 需要大量的内存，空间复杂度高。
- 对于高维数据，需要进行降维等预处理操作，以避免“维数灾难”问题。在高维空间中，距离度量会变得不可靠，因为大部分样本之间的距离都变得相近，这种现象被称为“维数灾难”。
- 对于不平衡的数据集，可能会导致预测精度低下。K近邻算法中每个样本的权重都相同，因此在不平衡的数据集中，K近邻算法可能会受到一些样本数量较少的类别的影响，从而导致预测精度低下。

### 3.3 常见问题

**k值一般取多大？有什么影响？**

- 一般而言，K值的大小需要根据具体的问题来选择。在选择K值时，需要考虑以下几个因素：
  - 样本数量：当样本数量较大时，可以适当增大K值，以减少噪声对分类的影响。
  - 数据分布：当数据分布比较简单，即类别之间的边界比较明显时，K值可以选择较小的值；当数据分布比较复杂，即类别之间的边界比较模糊时，K值需要选择较大的值。
  - 计算成本：K值越大，算法的计算成本越高，因为需要计算更多的距离。当数据集较大时，需要权衡计算成本和分类性能，选择一个适当的K值。

- 当**K值取小了**时，算法的效果通常会变得更加敏感，即对于噪声和异常值的容忍度变小，从而可能导致算法的过拟合。此时算法的决策边界会变得更加复杂，可能会导致算法过度拟合训练集的数据，从而在测试集上表现不佳。因此，在选择K值时，需要避免选择过小的值。
- 当**K值取大了**时，算法的效果通常会变得更加平滑，即对于噪声和异常值的容忍度变大，从而可能导致算法的欠拟合。此时算法的决策边界会变得更加简单，可能会忽略掉训练集中的一些细节，从而在测试集上表现不佳。因此，在选择K值时，需要避免选择过大的值。

**近似误差与估计误差**

- **近似误差**可以理解为对现有训练集的训练误差。近似误差关注于训练集，如果近似误差小了会出现过拟合的现象，对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大偏差的预测。模型本身不是最接近最佳模型。
- **估计误差**可以理解为对测试集的测试误差。估计误差关注测试集，估计误差小了说明对未知数据的预测能力好。模型本身最接近最佳模型。

**如何评估K近邻算法的性能？有哪些常用的评估指标？**

评估K近邻算法的性能可以使用各种评估指标，常用的评估指标包括以下几种：

- 精度（Accuracy）：分类正确的样本数占总样本数的比例。
- 召回率（Recall）：被正确分类的正例样本数占总正例样本数的比例。
- 精确率（Precision）：被正确分类的正例样本数占被分类为正例样本数的比例。
- F1值（F1 Score）：综合考虑精确率和召回率的一个指标，取值范围为0到1。
- ROC曲线（ROC Curve）：以假正例率为横坐标，真正例率为纵坐标，绘制出的曲线。
- AUC（Area Under Curve）：ROC曲线下的面积，取值范围为0.5到1，值越大，算法性能越好。

**K近邻算法在工业界有哪些应用场景？如何应用K近邻算法解决实际问题？**

K近邻算法在工业界有许多应用场景，以下是其中几个常见的场景：

- 推荐系统：K近邻算法可以根据用户的历史行为和偏好，推荐与其相似的产品或服务。
- 图像识别：K近邻算法可以将新的图像与已有的图像进行比较，找到最相似的图像，并对其进行分类。
- 文本分类：K近邻算法可以根据文本的相似度，将新的文本进行分类，例如情感分析、垃圾邮件过滤等。
- 医疗诊断：K近邻算法可以根据病人的病例信息，将其与已有的病例进行比较，找到最相似的病例，并给出相应的诊断结果。

**在K近邻算法中，如何处理样本特征的不同尺度问题？是否有必要对特征进行归一化？**

在K近邻算法中，样本特征的不同尺度问题会影响到距离的计算结果。如果某些特征的尺度较大，那么在计算距离时，这些特征所占的权重也会更大，从而影响到最终的分类结果。因此，对于不同尺度的特征，在进行距离计算之前，需要将其进行归一化处理，以消除不同尺度之间的影响。

通常采用的归一化方法有两种：最小-最大规范化和Z-score标准化。

- 最小-最大规范化（Min-Max Normalization）：将特征值缩放到[0,1]之间的区间内。具体做法是对于每个特征，将其原始值减去最小值，再除以最大值和最小值之差。公式如下：

  $x' = (x - min(x)) / (max(x) - min(x))$

  其中，x'为归一化后的值，x为原始值，min(x)和max(x)分别为该特征的最小值和最大值。

- Z-score标准化（Standardization）：将特征值转换为均值为0，标准差为1的分布。具体做法是对于每个特征，将其原始值减去平均值，再除以标准差。公式如下：

  $x' = (x - mean(x)) / std(x)$

  其中，x'为归一化后的值，x为原始值，mean(x)和std(x)分别为该特征的平均值和标准差。

对于不同尺度的特征，一般采用以上方法之一进行归一化处理。这样可以保证不同尺度的特征对距离的贡献相同，从而提高模型的分类准确率。因此，在使用K近邻算法时，对特征进行归一化是非常必要的。